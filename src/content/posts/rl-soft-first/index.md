---
title: 基于pybullet的软体机器人仿真与强化学习反运动学学习
published: 2025-10-02
description: "一份简单的技术报告"
image: "./a.gif"
tags: [simulation,robotics,Reinforcement learning]
category: Technical Report
draft: false
---

## 前言

这个项目是我大二下做的一个项目，当时刚刚联系到老师，老师说让我先了解一下连续体机器人这个领域，然后在这个领域开始做一些工作，当时他给我的技术路线就是使用pybullet来做这个仿真。当然了后来老师也说这个仿真和现实根本没有关系，现实机器人不可能是这么做的，这个逆运动学的方式也和强化学习马尔科夫决策链的形式不是很符合，因此这个项目其实没有什么实际的价值。但是这个项目确实给了一个还不错的技术路线，就是从仿真到策略训练到整体的代码的编写规范等等，还是起到了一点很不错的效果的。

然后说一下之后可能不太会提到的点，首先我必须承认这个代码在AI的帮助下完成，这也是无可厚非的，在AI时代不使用AI只是对自己效率的降低，但是我必须要说现在的AI还不能做到100%完全托管，像是cursor使用的就很难受。但是chatgpt那种形式就还可以。

代码的内容开源在[github](https://github.com/Tosania/pybullet-based-control-of-continuum-robots-using-RL)上。

## 仿真部分

这个仿真主要基于分段等曲率(PCC)实现，具体来讲，整个模型共有六个自由度：伸缩自由度、三段的转动和角度自由度。让我们从底层的urdf实现来说起。

### urdf

源文件位于

> source/continuum_robot.urdf

在pybullet之中，我们没有办法通过直接的手段来做一个软体杆——这是十分可惜的，因为如果能够直接模拟一根软体弹性杆的包括力学性质在内的东西无疑会让我们有更加简单的仿真。这里我们使用的是利用多段短的刚体段连接形成类似柔体的效果。其中中间的连接关节设置为type="continuous"，然后为了仿真实际的外部内容加上了外部圆片。这一部分内容并不是技术难点。

底座部分，我们让三段等曲率的末端和底座上的一个可上下滑动的点连接，以此达到上下运动的效果。这一部分将在之后实现上下滑动算法部分体现。

值得说明的是urdf部分应当由python自动生成，不然30个段还是挺多的。

### simulation设计

#### 基本实现

整体上我们将实现以下功能：1.设计一套简单的通用数据格式。2.实现从底层到高封装的运动学，3.实现两种不同的运动学控制。4.实现相关数据的获取。我们将一一介绍。

#### 数据格式：

整体上数据格式分为几个部分，首先是各个等曲率段的设计，在代码中主要为

````python
class arm_joint:
    in_control_joint: List[int]  # 存储数组
    first_control_joint: int         # 第一个数字
    direction: int      # 第二个数字（可以是浮点数）
````

第一个部分为统辖的关节坐标，之后为第一个关节，由于等曲率变化会对第一个产生特殊的影响所以分开，之后是整体的朝向。

之后是描述整个机器人的若干参数，主要为

```
state = [
          self.theta_1, self.alpha_1,
          self.theta_2, self.alpha_2,
          self.theta_3, self.alpha_3,
          self.base_theta, self.length
      ]
```

还有若干用于初始化相关机器人内部关节数目或是具体下标的内容，具体详见代码。

#### 运动学

运动学主要要从底层到上层的各种控制，首先是底层，我们这里没有选择使用普通pybullet自带的关节控制而是使用reset，所以会直接锁死关节（这其实是为了简化才做的)

之后就是对整段的控制，也就是set_arm，这里具体的实现上主要是1.考虑到伸缩的功能，也就是我们将一部分关节缩到底座内部，那么我们就需要让那部分不再被控制，转而是零度角，同时要重新设计一号角。2，第一个关节的角度弯曲是不同于其他关节的，具体来说他只有其他关节的一半弯曲。3.要注意边界条件。

#### 两种运动学的控制

所谓两种运动学的控制，一是运动学直接控制，二是基于速度的控制。运动学的直接控制指的是直接使用各个自由度然后控制，也就是原代码里面的set_robot函数，这个控制是我之后的主要控制方法。

基于速度的控制还是一个正在进行中的运动，具体来说他会控制每个自由度的倒数，同时限制边界条件，也就是源代码里的apply_velocity_control。这一部分还相当的原始，之后的训练中效果比较一般，因此我认为这一部分还需要更多的尝试。

#### 数据获取

这里主要有两个部分，一是我在之后主要涉及末端控制，所以我需要获取整个机器人的末端位置用于检验和状态获取。二是我可能需要一部分机体的参数（这是值得说明的，在之后训练的状态部分我们会具体说明）。最后是我需要一个目标位置。

第一部分还是比较简单的，也就是原代码中的get_end_effector_position。第二部分也不难，我们会在之前预处理出需要的关节列表，然后直接输出就可以了。

最后，我们采用将随机位置可视化的方法。值得说明的是一定要采用正确的手段让每一次小球使用完成后删除，不然会造成严重的内存泄漏。

## 训练部分

OK，基本的simulation已经完成，之后我们将使用NVIDIA的gym和stable-baseline3来完成强化学习的训练。

#### gym相关

gym相关主要是为强化学习配置一些简单的范围。首先是state范围，直接到达的state注意一下范围就行。这里在后续会出现一个思考，就是是否是state越复杂越好，对于这一部分，首先我们至少需要能用整个state来复现这个软体机器人相关的内容，但是冗余内容要不要我还没实验出来。然后是action，这个注意一下速度大小和普通的边界就可以了。

然后是随机点生成，注意一下我的算法，这一部分一开始我是用球来生成的，但是无疑这样会造成无法到达点的问题。所以后来我采用正向运动学随机生成点的方式来生成随机点，这里后续再相关介绍。

还有记录相关，我这里主要记录50步内的平均reward，还有检测是否完成，这里的检测函数得自己写，不同的区别很大，之后在慢慢详细介绍。还有reward相关，之后会详细介绍。

#### trainer

为了结构化，单次训练过程我就写成trainer了。具体参数量有点大，得看代码，主要参数有:

model_type:还是ppo好用。

total_timesteps:训练步数，大了可能会过拟合，但是我还是保留了最优秀的模型，所以尽量大一些不是问题（

batch_size:批大小，具体得多调一调，和过拟合有点关系，大了之后容易过拟合，小了容易学不到

n_envs/n_steps:具体得参见强化学习训练过程，这个和并行有关系，基本上cpu有多少核心，就奔着核心数去，如果核心比较好可以设置到1.5到2倍。

其它参数可以自己试一试。

这个训练使用机器注意一下，由于组里还没有服务器，所以我用的autodl，一般来说主要需要一个大一点的cpu，内存一般在本地也不是很好搞，可能需要开一些虚拟内存。一般一次训练大概一个小时。

#### train

这是核心训练不分，支持多训练环境串行，具体看实现代码，可以自定义相关的reward和done函数。

## 配套工具部分

#### 小球预处理

为了使小球一定可以到达，我们设计了一个预处理小球批次得到。可以通过keshi来实现并可视，其中大批量用作测试，小批量用于过程检验

#### call_back和tensorboard

这一部分主要是为了在训练完成后观看效果的，call_back主要是利用小批量检验模型中途效果，用以保存测试机最优。

#### pdf_generate

这一工具主要用于自动生成最终的pdf报告了，具体看相关代码。

#### demo

可用于生成演示效果

#### control

这个github上面没上传，主要是可以手动遥控整个机器人。

## 最终效果解析部分

由于这个技术报告撰写的时候我已经好久没有做过这个项目了，所以很多原始数据早就丢失了。我这里大概回忆一下几个核心的效果：

1.最优效果，基本上就是报告.PDF指出的那样，后续有一些小的提升，但是没有实质性的变化。

2.核心问题：batch_size相关，reward相关，done相关。

具体的我在那边应该留了一点相关的数据可以看一下。
